{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-6-e6db081213c4>, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-e6db081213c4>\"\u001b[1;36m, line \u001b[1;32m21\u001b[0m\n\u001b[1;33m    'kernel_initializer=\"he_normal\"''),\u001b[0m\n\u001b[1;37m                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "참조\n",
    "generator = keras.models.Sequential([\n",
    "    keras.layers.Dense(7 * 7 * 128, input_shape=[codings_size]),\n",
    "    keras.layers.Reshape([7, 7, 128]),\n",
    "    keras.layers.Conv2DTranspose(64, kernel_size=5, strides=2, padding=\"SAME\",\n",
    "                                 activation=\"selu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2DTranspose(1, kernel_size=5, strides=2, padding=\"SAME\",\n",
    "                                 activation=\"tanh\"),\n",
    "])\n",
    "\n",
    "'''\n",
    "class ResidualBlock(keras.layers.Layer):\n",
    "    def __init__(filters=3, kernel_size=64,strides=1,padding,activation='PReLU'):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Conv2D(filters=filters, kerner_size=kernel_size, stride=stride, activation=\"PReLU\", 'kernel_initializer=\"he_normal\"'),\n",
    "                       keras.layers.BatchNormalization(act=tf.nn.relu,'??gama'),\n",
    "                       keras.layers.Conv2D(filters=filters, kerner_size=kernel_size, stride=stride, activation=\"PReLU\", 'kernel_initializer=\"he_normal\"''),\n",
    "                       keras.layers.BatchNormalization('??gama')\n",
    "                       ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z\n",
    "        \n",
    "class GENERATOR(keras.models.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.front1 = keras.layers.Conv2D(filters=9, kerner_size=64, stride=1, activation=\"PReLU\",\n",
    "                                          'kernel_initializer=\"he_normal\"')\n",
    "        self.block1 = ResidualBlock(?, ?)\n",
    "        \n",
    "        self.back1 = Keras.models.Sequential([\n",
    "        keras.layers.Conv2D(filters=3, kerner_size=64, stride=1, activation=\"PReLU\", 'kernel_initializer=\"he_normal\"'), \n",
    "        keras.layers.BatchNormalization(act=tf.nn.relu,'??gama')])\n",
    "        \n",
    "        self.back2 = keras.models.Sequential([keras.layers.Conv2D(filters=3, kerner_size=256, stride=1, activation=\"PReLU\", 'kernel_initializer=\"he_normal\"'), \n",
    "        SubpixelConv2d(scale=2, n_out_channels=None, act=tf.nn.relu)])\n",
    "        \n",
    "        self.back3 = keras.models.Sequential([keras.layers.Conv2D(filters=3, kerner_size=256, stride=1, activation=\"PReLU\", 'kernel_initializer=\"he_normal\"'), \n",
    "        SubpixelConv2d(scale=2, n_out_channels=None, act=tf.nn.relu)])\n",
    "        \n",
    "        self.back4 = keras.models.Sequential([keras.layers.Conv2D(filters=9, kerner_size=3, stride=1, activation=\"PReLU\", 'kernel_initializer=\"he_normal\"')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = self.front1(inputs)\n",
    "        Z1= self.front1(inputs)# calculate?  Z.copy?\n",
    "        for _ in range(0,5):\n",
    "            Z = self.block1(Z)\n",
    "        Z=self.back1(Z)\n",
    "        Z2=Z1+Z\n",
    "        Z2=self.back2(Z2)\n",
    "        Z2=self.back3(Z2)\n",
    "        Z2=self.back4(Z2)\n",
    "        return Z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DISCRIMINATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Keras.models.Sequential([\n",
    "        keras.layers.Conv2D(filters=3, kerner_size=64, stride=1, activation=\"LeakyLeRU\", 'kernel_initializer=\"he_normal\"'),\n",
    "        keras.layers.Conv2D(filters=3, kerner_size=64, stride=2, activation=\"LeakyLeRU\", 'kernel_initializer=\"he_normal\"'),\n",
    "        keras.layers.BatchNormalization(act=tf.nn.relu,'??gama'),\n",
    "        keras.layers.Conv2D(filters=3, kerner_size=128, stride=1, activation=\"LeakyLeRU\", 'kernel_initializer=\"he_normal\"'),\n",
    "        keras.layers.BatchNormalization(act=tf.nn.relu,'??gama'),\n",
    "        keras.layers.Conv2D(filters=3, kerner_size=128, stride=2, activation=\"LeakyLeRU\", 'kernel_initializer=\"he_normal\"'),\n",
    "        keras.layers.BatchNormalization(act=tf.nn.relu,'??gama'),\n",
    "        keras.layers.Conv2D(filters=3, kerner_size=256, stride=1, activation=\"LeakyLeRU\", 'kernel_initializer=\"he_normal\"'),\n",
    "        keras.layers.BatchNormalization(act=tf.nn.relu,'??gama'),\n",
    "        keras.layers.Conv2D(filters=3, kerner_size=256, stride=2, activation=\"LeakyLeRU\", 'kernel_initializer=\"he_normal\"'),\n",
    "        keras.layers.BatchNormalization(act=tf.nn.relu,'??gama'),\n",
    "        keras.layers.Conv2D(filters=3, kerner_size=512e, stride=1, activation=\"LeakyLeRU\", 'kernel_initializer=\"he_normal\"'),\n",
    "        keras.layers.BatchNormalization(act=tf.nn.relu,'??gama'),\n",
    "        keras.layers.Conv2D(filters=3, kerner_size=512, stride=2, activation=\"LeakyLeRU\", 'kernel_initializer=\"he_normal\"'),\n",
    "        keras.layers.BatchNormalization(act=tf.nn.relu,'??gama'),\n",
    "        keras.layers.Dense(1024,activation='LeakyLeRU', kernel_initializer=\"he_normal\"),\n",
    "        #keras.layers.LeakyReLU()??\n",
    "        keras.layers.Dense(1,activation=sigmoid, kernel_initializer=\"he_normal\")\n",
    "        ]) \n",
    "        \n",
    "        \n",
    "model.compile()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
